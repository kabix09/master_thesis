{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01a8400-5267-4dd5-a990-dee8d8f666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# np.float_ = np.float64\n",
    "# __import__('pysqlite3')\n",
    "# import sys\n",
    "# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Losowanie k prac\n",
    "np.random.seed(42)  # Ustawienie ziarna losowości dla powtarzalności wyników"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec770c-5ba2-4c9d-9f40-48ce2b29d942",
   "metadata": {},
   "source": [
    "## Distribution generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5595ea10-c0d9-4765-8bb4-ea4c04b3b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualAggregator:\n",
    "    \"\"\"\n",
    "    Generates a distribution of selected papers based on specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of citations to sample.\n",
    "    N : int\n",
    "        Page size for pagination.\n",
    "    p : list\n",
    "        List of weights for criteria: [semantic similarity, publication year, number of citations, publication venue].\n",
    "    Q : str\n",
    "        Query used for selecting papers.\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing query results with columns: 'id', 'title', 'similarity', 'year', 'n_citation', 'gov_score'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Counter\n",
    "        Counter object containing identifiers of selected papers and their counts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.collection = None\n",
    "        self.N = None\n",
    "        self.k = None\n",
    "        self.pn = None\n",
    "        self.chroma_collection = None\n",
    "        self.init_connection()\n",
    "\n",
    "    def set_parameters(self, N, k, pn):\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "        self.pn = pn\n",
    "\n",
    "    def init_connection(self):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                chroma_client = chromadb.PersistentClient(path=\"../data/chroma\")\n",
    "                #chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "                self.chroma_collection = chroma_client.get_or_create_collection(name=\"articles_with_score\")\n",
    "                collection_status = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def get_similar_articles(self, query_embedding, max_similarities):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                return self.chroma_collection.query(\n",
    "                    query_embeddings=[query_embedding],  # Zanurzenie zapytania\n",
    "                    n_results=max_similarities  # Liczba zwracanych wyników\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def distribution_function(self, page_count):\n",
    "        pages_distribution = np.exp(-np.arange(1, page_count + 1))\n",
    "        pages_distribution /= pages_distribution.sum()\n",
    "        return pages_distribution\n",
    "\n",
    "    def distribution_generator(self, collection_dict):    \n",
    "        values_to_scale = np.array([\n",
    "                collection_dict['year'],\n",
    "                collection_dict['n_citation'],\n",
    "                collection_dict['gov_score']\n",
    "            ]).T\n",
    "\n",
    "        # Dopasowanie i przekształcenie danych\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_values = scaler.fit_transform(values_to_scale)\n",
    "\n",
    "        collection_dict['year_normalized'] = scaled_values[:, 0].tolist()\n",
    "        collection_dict['citations_normalized'] = scaled_values[:, 1].tolist()\n",
    "        collection_dict['points_normalized'] = scaled_values[:, 2].tolist()\n",
    "\n",
    "        collection_dict['score'] = [\n",
    "            self.pn[0] * collection_dict['similarity'][i] +\n",
    "            self.pn[1] * collection_dict['year_normalized'][i] +\n",
    "            self.pn[2] * collection_dict['citations_normalized'][i] +\n",
    "            self.pn[3] * collection_dict['points_normalized'][i]\n",
    "            for i in range(len(collection_dict['id']))\n",
    "        ]\n",
    "\n",
    "        # Tworzenie listy słowników dla posortowania\n",
    "        sorted_collection = sorted(\n",
    "            [\n",
    "                {\n",
    "                    'id': collection_dict['id'][i],\n",
    "                    'title': collection_dict['title'][i],\n",
    "                    'similarity': collection_dict['similarity'][i],\n",
    "                    'year': collection_dict['year'][i],\n",
    "                    'n_citation': collection_dict['n_citation'][i],\n",
    "                    'gov_score': collection_dict['gov_score'][i],\n",
    "                    'year_normalized': collection_dict['year_normalized'][i],\n",
    "                    'citations_normalized': collection_dict['citations_normalized'][i],\n",
    "                    'points_normalized': collection_dict['points_normalized'][i],\n",
    "                    'score': collection_dict['score'][i]\n",
    "                }\n",
    "                for i in range(len(collection_dict['id']))\n",
    "            ],\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Stronicowanie wyników\n",
    "        ranked_indices = [entry['id'] for entry in sorted_collection]\n",
    "\n",
    "        selected_papers = []\n",
    "        for _ in range(sekf.k):\n",
    "            selected_papers.append(ranked_indices[_])\n",
    "\n",
    "        return collections.Counter(selected_papers)\n",
    "\n",
    "        # pages = [ranked_indices[i:i + self.N] for i in range(0, len(ranked_indices), self.N)]\n",
    "        # pages_distribution = self.distribution_function(len(pages))\n",
    "\n",
    "        # last_iter_size = len(pages)\n",
    "        # non_empty_pages = pages\n",
    "\n",
    "        # selected_papers = []\n",
    "        # for _ in range(self.k):\n",
    "        #     if self.k > self.N and _ >= self.N:\n",
    "        #         # Problem pustej strony - pojawia sie kiedy zdejmiemy juz wszytskei dostepne artykuły z tej strony w drodze losowania bez powtórzeń\n",
    "        #         non_empty_pages = [page for page in pages if len(page) > 0]\n",
    "        #         if len(non_empty_pages) != last_iter_size:\n",
    "        #             pages_distribution = self.distribution_function(len(non_empty_pages))\n",
    "        #             last_iter_size = len(non_empty_pages)\n",
    "\n",
    "        #     selected_page_index = np.random.choice(len(non_empty_pages), p=pages_distribution)\n",
    "        #     selected_page = non_empty_pages[selected_page_index]                \n",
    "        #     selected_paper_index = np.random.choice(selected_page)\n",
    "        #     selected_papers.append(selected_paper_index)\n",
    "\n",
    "        #     # Usuwanie wylosowanych wyników\n",
    "        #     pages[selected_page_index] = [x for x in selected_page if x != selected_paper_index]\n",
    "\n",
    "        # Zapisanie identyfikatorów wylosowanych prac\n",
    "        selected_paper_counts = collections.Counter(selected_papers)\n",
    "\n",
    "        return selected_paper_counts\n",
    "\n",
    "    def select_papers(self, ranking):\n",
    "        selected_papers = random.sample(ranking, self.k)\n",
    "        return selected_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32263551-a67c-4fe7-81e6-b65aca0c610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, settings):\n",
    "        self.virtual_aggregator = VirtualAggregator()\n",
    "        self.queries = None\n",
    "        self.settings = settings\n",
    "        self.similar_articles = None\n",
    "\n",
    "    def run_experiment(self, batch=850000):\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Start\")\n",
    "        \n",
    "        self.load_queries()\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Loaded: {len(self.queries)} queries\")\n",
    "\n",
    "        display(self.healt_check()[0])\n",
    "        already_saved = int(self.healt_check()[0]['controll_sum'])\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Already saved: {already_saved} results\")\n",
    "\n",
    "        distribution_dict = {}\n",
    "\n",
    "        counter = 0\n",
    "        result_dict = {}\n",
    "\n",
    "        start_index = 0 + already_saved  # Ostatni indeks w liście\n",
    "        end_index = start_index + batch \n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Start from {start_index} to {end_index}\")\n",
    "\n",
    "        for query_id, query in enumerate(tqdm(self.queries[start_index:end_index], total=batch, desc=\"Queries\", unit=\"query\")):\n",
    "            self.similar_articles = self.virtual_aggregator.get_similar_articles(query.tolist(), 250)\n",
    "\n",
    "            for sample_id, sample in enumerate(self.settings):\n",
    "                self.virtual_aggregator.set_parameters(sample['N'], sample['k'], sample['pn'])\n",
    "                step_distribution = self.step()\n",
    "                \n",
    "                # Save result\n",
    "                if sample_id not in result_dict:\n",
    "                    # Tworzenie nowych list dla nowego sample_id\n",
    "                    result_dict[sample_id] = {\n",
    "                        'query_id': [start_index + query_id],\n",
    "                        'distribution': [dict(step_distribution)]\n",
    "                    }\n",
    "                else:\n",
    "                    # Poszerzanie istniejących list\n",
    "                    result_dict[sample_id]['query_id'].append(start_index + query_id)\n",
    "                    result_dict[sample_id]['distribution'].append(dict(step_distribution))\n",
    "                    \n",
    "                if str(sample) in distribution_dict:                  \n",
    "                    distribution_dict[str(sample)].update(step_distribution)\n",
    "                else:\n",
    "                    distribution_dict[str(sample)] = step_distribution\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter % 500 == 0:\n",
    "                self.save_distribution(distribution_dict)\n",
    "                #self.save_results(result_dict)\n",
    "                result_dict = {}\n",
    "    \n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Final distribution saving\")\n",
    "        self.save_distribution(distribution_dict)\n",
    "\n",
    "        #print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Final result saving\")\n",
    "        #self.save_results(result_dict)\n",
    "\n",
    "    def step(self):\n",
    "        collection_dict = {\n",
    "            'id': self.similar_articles['ids'][0],\n",
    "            'title': self.similar_articles['documents'][0],\n",
    "            'similarity': self.similar_articles['distances'][0],\n",
    "            'year': [metadata['year'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'n_citation': [metadata['n_citation'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'gov_score': [metadata['gov_score'] for metadata in self.similar_articles['metadatas'][0]]\n",
    "        }\n",
    "\n",
    "        ranked_indices = [int(entry) for entry in collection_dict['id']]\n",
    "        selected_papers = []\n",
    "        for _ in range(25):\n",
    "            selected_papers.append(ranked_indices[_])\n",
    "        return collections.Counter(selected_papers)\n",
    "        # self.virtual_aggregator.distribution_generator(collection_dict)\n",
    "\n",
    "    def load_queries(self):\n",
    "        df_query = pd.read_pickle('..//data//interim//queries_with_embeddings.pkl')[['embedding']]\n",
    "        \n",
    "        # Konwersja z użyciem tqdm\n",
    "        self.queries = df_query['embedding'].tolist()\n",
    "\n",
    "        df_query = None\n",
    "        del df_query\n",
    "        gc.collect()\n",
    "\n",
    "    def save_results(self, result_dict):\n",
    "        for settings_id, data in result_dict.items():\n",
    "            directory = f'../data/results/{settings_id}'\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "            # Ścieżka do pliku CSV\n",
    "            file_path = f'{directory}/results_primary_full.csv'\n",
    "            file_exists = os.path.isfile(file_path)\n",
    "            \n",
    "            # Otwieramy plik CSV na początku pętli, żeby pisać dane w jednej operacji\n",
    "            with open(file_path, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['query_id', 'distribution']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "        \n",
    "                # Teraz iterujemy bezpośrednio po danych\n",
    "                for i in range(len(data['query_id'])):\n",
    "                    query_id = data['query_id'][i]\n",
    "                    distribution = data['distribution'][i]\n",
    "                    \n",
    "                    writer.writerow({\n",
    "                        'query_id': query_id,\n",
    "                        'distribution': distribution\n",
    "                    })\n",
    "        #del writer\n",
    "\n",
    "    def save_distribution(self, distribution_dict):\n",
    "        distribution_df = pd.DataFrame(list(distribution_dict.items()), columns=['settings', 'distribution'])\n",
    "        distribution_df['distribution'] = distribution_df['distribution'].apply(lambda x: dict(x))\n",
    "        distribution_df.to_csv('../distributions_primary_full.csv')\n",
    "\n",
    "    def healt_check(self):\n",
    "        # Ścieżka do katalogu results\n",
    "        base_path = '../data/results/'\n",
    "\n",
    "        # Inicjalizacja słownika\n",
    "        results_dict = {}\n",
    "\n",
    "        # Pętla po wszystkich możliwych wartościach X\n",
    "        for x in tqdm(range(len(self.settings)), desc=\"Processing result files\"):\n",
    "            file_path = os.path.join(base_path, str(x), 'results_primary_full.csv')\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    # Zainicjalizuj zmienną licznika\n",
    "                    line_count = 0\n",
    "                    # Iteruj po każdej linii w pliku\n",
    "                    for line in file:\n",
    "                        line_count += 1\n",
    "                    \n",
    "                    # Odjąć nagłówek\n",
    "                    line_count -= 1\n",
    "                    \n",
    "                    # Dodanie do słownika\n",
    "                    results_dict[x] = {'controll_sum': line_count}\n",
    "            except FileNotFoundError:\n",
    "                results_dict[x] = {'controll_sum': 0}\n",
    "\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4214a120-46a2-4b5a-967f-3e494f996a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_settings():\n",
    "    # Rozmiar paginy\n",
    "    page_sizes = [100]\n",
    "\n",
    "    # Liczba cytowań\n",
    "    citation_numbers = [25]\n",
    "\n",
    "    # Możliwe wartości wag A, B, C, D\n",
    "    weights = [0., 0.25, 1.0]\n",
    "\n",
    "    # Generowanie wszystkich możliwych kombinacji wag\n",
    "    all_combinations = list(itertools.product(weights, repeat=4))\n",
    "\n",
    "    # Filtrowanie kombinacji, aby suma wag wynosiła między 0.99 a 1.0\n",
    "    valid_configs = [list(c) for c in all_combinations if 0.99 <= sum(c) <= 1.0]\n",
    "\n",
    "    # Generowanie wszystkich możliwych ustawień\n",
    "    settings = []\n",
    "    for page_size in page_sizes:\n",
    "        for citation_number in citation_numbers:\n",
    "            for config in valid_configs:\n",
    "                settings.append({\n",
    "                    'N': page_size,\n",
    "                    'k': citation_number,\n",
    "                    'pn': config\n",
    "                })\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bdb7f-f9dd-4d29-928f-325f1bf8e6b8",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4186-07d9-4707-870d-5111933552fd",
   "metadata": {},
   "source": [
    "### 1. Generate settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ecdf0ea-1650-4919-a5bc-efad413ea86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'N': 100, 'k': 25, 'pn': [0.0, 0.0, 0.0, 1.0]},\n",
       " {'N': 100, 'k': 25, 'pn': [0.0, 0.0, 1.0, 0.0]},\n",
       " {'N': 100, 'k': 25, 'pn': [0.0, 1.0, 0.0, 0.0]},\n",
       " {'N': 100, 'k': 25, 'pn': [0.25, 0.25, 0.25, 0.25]},\n",
       " {'N': 100, 'k': 25, 'pn': [1.0, 0.0, 0.0, 0.0]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'N': 100, 'k': 25, 'pn': [0.0, 0.0, 0.0, 1.0]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "settings = generate_all_settings()\n",
    "display(settings)\n",
    "\n",
    "settings = [settings[0]]\n",
    "display(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cecd8d9-0e97-45a1-b7e0-8feaaa55ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = '../data/settings_primary.pkl'\n",
    "\n",
    "# Zapis do pliku .pkl\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(settings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e853a3f-6c9e-4141-a8fd-ac102aebdbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 100, 'k': 25, 'pn': [0.0, 0.0, 0.0, 1.0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wygenerowanych konfiguracji:  4\n"
     ]
    }
   ],
   "source": [
    "settings = pd.read_pickle('../data/settings_primary.pkl')\n",
    "display(settings[0])\n",
    "print(\"Liczba wygenerowanych konfiguracji: \", len(settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076b988-101a-401b-bd54-2297ffbdcf04",
   "metadata": {},
   "source": [
    "### 2. Run main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3e90856-e23e-4f4f-93a2-c1c99cb8ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 15:46:59 Start\n",
      "2024-08-27 15:47:05 Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing result files: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'controll_sum': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing result files: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1676.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 15:47:05 Already saved: 0 results\n",
      "2024-08-27 15:47:05 Start from 0 to 850000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries: 100%|████████████████████████████████████████████████████████████| 850000/850000 [3:13:59<00:00, 73.02query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 19:01:04 Final distribution saving\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(settings)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa475623-1ba9-4bee-adff-cbae0eb8a3ea",
   "metadata": {},
   "source": [
    "###### 3. Read result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbcf2bf-90be-42df-be98-bd44e3e120e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>settings</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'N': 100, 'k': 25, 'pn': [0.0, 0.0, 0.0, 1.0]}</td>\n",
       "      <td>{378886: 151, 164289: 119, 219194: 280, 285797...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         settings  \\\n",
       "0           0  {'N': 100, 'k': 25, 'pn': [0.0, 0.0, 0.0, 1.0]}   \n",
       "\n",
       "                                        distribution  \n",
       "0  {378886: 151, 164289: 119, 219194: 280, 285797...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_df = pd.read_csv('../distributions_primary_full.csv')\n",
    "display(distribution_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568afe8c-5999-4829-bcf5-f6fe9d5e6083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wygenerowanych konfiguracji:  306\n"
     ]
    }
   ],
   "source": [
    "path = '../data/distribution_merged.csv'\n",
    "settings_pkl = '../data/settings.pkl'\n",
    "settings = pd.read_pickle(settings_pkl)\n",
    "display(settings[0])\n",
    "print(\"Liczba wygenerowanych konfiguracji: \", len(settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167160fc-3353-49a8-a190-2c7344357249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing merged results:   0%|                                                               | 0/306 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust this path as necessary\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43madd_settings_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36madd_settings_column\u001b[1;34m(base_path, settings, max_x)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Map settings_id to their corresponding settings from the pkl file\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msettings_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m display(fd)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4680\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[0;32m   4601\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4602\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[0;32m   4603\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4604\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4605\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4606\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4607\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4678\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4680\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4682\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4683\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_settings_column(base_path, settings, max_x=305):\n",
    "    # Iterate over all directories from 0 to max_x\n",
    "    for x in tqdm(range(0, max_x + 1), desc=\"Processing merged results\"):\n",
    "        # Define path for merged_results\n",
    "        merged_results_path = os.path.join(base_path, f'distribution_merged.csv')\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(merged_results_path):\n",
    "            print(f'Skipping {x} as the file is missing')\n",
    "            continue\n",
    "\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(merged_results_path)\n",
    "\n",
    "        # Ensure the settings_id column exists\n",
    "        if 'settings_id' not in df.columns:\n",
    "            print(f'Skipping {x} as settings_id column is missing')\n",
    "            continue\n",
    "\n",
    "        # Map settings_id to their corresponding settings from the pkl file\n",
    "        df['settings'] = df['settings_id'].map(settings)\n",
    "\n",
    "        display(fd)\n",
    "        break\n",
    "        # # Save the updated DataFrame back to CSV (optional: you can choose to overwrite or save as new file)\n",
    "        # output_path = os.path.join(base_path, f'merged_results/{x}/merged_results_with_settings.csv')\n",
    "        # df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f'Updated DataFrame saved with settings for settings_id {x} at {output_path}')\n",
    "\n",
    "# Example usage:\n",
    "base_path = '../data'  # Adjust this path as necessary\n",
    "add_settings_column(base_path, settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69764559-5c72-46be-98d6-5b37a9790572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
