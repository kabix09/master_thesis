{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01a8400-5267-4dd5-a990-dee8d8f666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import tempfile\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import cpu_count, set_start_method #, Pool\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec770c-5ba2-4c9d-9f40-48ce2b29d942",
   "metadata": {},
   "source": [
    "## Function generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c19690-6f5f-401c-9afa-039e60ebaae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243aa769-25b8-4a18-9def-b7d5b32c0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualAggregator:\n",
    "    def __init__(self):\n",
    "        self.collection = None\n",
    "        self.N = None\n",
    "        self.k = None\n",
    "        self.pn = None\n",
    "        self.chroma_collection = None\n",
    "        self.init_connection()\n",
    "\n",
    "    def set_parameters(self, N, k, pn):\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "        self.pn = pn\n",
    "\n",
    "    def init_connection(self):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "                self.chroma_collection = chroma_client.get_or_create_collection(name=\"articles_with_score\")\n",
    "                collection_status = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "\n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def get_similar_articles(self, query, k):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                return self.chroma_collection.query(query_texts=[query], n_results=2*k)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "\n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def distribution_function(self, page_count):\n",
    "        pages_distribution = np.exp(-np.arange(1, page_count + 1))\n",
    "        pages_distribution /= pages_distribution.sum()\n",
    "        return pages_distribution\n",
    "\n",
    "    def distribution_generator(self, collection_dict):\n",
    "        scaler = MinMaxScaler()\n",
    "        values_to_scale = np.array([\n",
    "            collection_dict['year'],\n",
    "            collection_dict['n_citation'],\n",
    "            collection_dict['gov_score']\n",
    "        ]).T\n",
    "\n",
    "        scaled_values = scaler.fit_transform(values_to_scale)\n",
    "        collection_dict['year_normalized'] = scaled_values[:, 0].tolist()\n",
    "        collection_dict['citations_normalized'] = scaled_values[:, 1].tolist()\n",
    "        collection_dict['points_normalized'] = scaled_values[:, 2].tolist()\n",
    "\n",
    "        collection_dict['score'] = [\n",
    "            self.pn[0] * collection_dict['similarity'][i] +\n",
    "            self.pn[1] * collection_dict['year_normalized'][i] +\n",
    "            self.pn[2] * collection_dict['citations_normalized'][i] +\n",
    "            self.pn[3] * collection_dict['points_normalized'][i]\n",
    "            for i in range(len(collection_dict['id']))\n",
    "        ]\n",
    "\n",
    "        sorted_collection = sorted(\n",
    "            [\n",
    "                {\n",
    "                    'id': collection_dict['id'][i],\n",
    "                    'title': collection_dict['title'][i],\n",
    "                    'similarity': collection_dict['similarity'][i],\n",
    "                    'year': collection_dict['year'][i],\n",
    "                    'n_citation': collection_dict['n_citation'][i],\n",
    "                    'gov_score': collection_dict['gov_score'][i],\n",
    "                    'year_normalized': collection_dict['year_normalized'][i],\n",
    "                    'citations_normalized': collection_dict['citations_normalized'][i],\n",
    "                    'points_normalized': collection_dict['points_normalized'][i],\n",
    "                    'score': collection_dict['score'][i]\n",
    "                }\n",
    "                for i in range(len(collection_dict['id']))\n",
    "            ],\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        ranked_indices = [entry['id'] for entry in sorted_collection]\n",
    "        pages = [ranked_indices[i:i + self.N] for i in range(0, len(ranked_indices), self.N)]\n",
    "        pages_distribution = self.distribution_function(len(pages))\n",
    "\n",
    "        np.random.seed(42)\n",
    "        selected_papers = []\n",
    "        for _ in range(self.k):\n",
    "            selected_page_index = np.random.choice(len(pages), p=pages_distribution)\n",
    "            selected_page = pages[selected_page_index]\n",
    "            selected_paper_index = np.random.choice(selected_page)\n",
    "            selected_papers.append(selected_paper_index)\n",
    "\n",
    "            pages[selected_page_index] = [x for x in selected_page if x != selected_paper_index]\n",
    "\n",
    "        selected_paper_counts = collections.Counter(selected_papers)\n",
    "        return selected_paper_counts\n",
    "\n",
    "#def process_query(query, settings, file_path):\n",
    "def process_query(args):\n",
    "    query, settings, file_path = args\n",
    "    virtual_aggregator = VirtualAggregator()\n",
    "    max_k = max(settings, key=lambda x: x['k'])['k']\n",
    "\n",
    "    result_dict = {\n",
    "        'title': [],\n",
    "        'settings': [],\n",
    "        'distribution': [],\n",
    "    }\n",
    "    print('process_query')\n",
    "    similar_articles = virtual_aggregator.get_similar_articles(query, max_k)\n",
    "\n",
    "    for sample in settings:\n",
    "        virtual_aggregator.set_parameters(sample['N'], sample['k'], sample['pn'])\n",
    "        distribution = step(query, similar_articles, virtual_aggregator)\n",
    "\n",
    "        # Save result\n",
    "        result_dict['title'].append(query)\n",
    "        result_dict['settings'].append(sample)\n",
    "        result_dict['distribution'].append(dict(distribution))\n",
    "\n",
    "    save_results(result_dict, file_path)\n",
    "\n",
    "def step(query, similar_articles, virtual_aggregator):\n",
    "    collection_dict = {\n",
    "        'id': similar_articles['ids'][0],\n",
    "        'title': similar_articles['documents'][0],\n",
    "        'similarity': similar_articles['distances'][0],\n",
    "        'year': [metadata['year'] for metadata in similar_articles['metadatas'][0]],\n",
    "        'n_citation': [metadata['n_citation'] for metadata in similar_articles['metadatas'][0]],\n",
    "        'gov_score': [metadata['gov_score'] for metadata in similar_articles['metadatas'][0]]\n",
    "    }\n",
    "\n",
    "    return virtual_aggregator.distribution_generator(collection_dict)\n",
    "\n",
    "def save_results(result_dict, file_path):\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    keys = result_dict.keys()\n",
    "    with open(file_path, 'a', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        if not file_exists:\n",
    "            dict_writer.writeheader()\n",
    "        dict_writer.writerows([dict(zip(keys, row)) for row in zip(*result_dict.values())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e701b3a-dbc9-4da1-864c-ab038b53a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process_query(args):\n",
    "    print('run_process_query')\n",
    "    query, settings, file_path = args\n",
    "    process_query(query, settings, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cc3b1-1e3c-482e-9300-20a176075c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev `../data/queries_df.csv` was generated in 3. but was replased with .pkl\n",
    "class Experiment:\n",
    "    def __init__(self, settings):\n",
    "        self.virtual_aggregator = VirtualAggregator()\n",
    "        self.settings = settings\n",
    "        self.queries = None\n",
    "         \n",
    "    def generate_arguments_with_progress(self, queries, settings, file_path):\n",
    "        def generate_for_query(query, settings, file_path):\n",
    "            return (query, settings, file_path)\n",
    "\n",
    "        results = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(generate_for_query, query, settings, file_path) for query in tqdm(queries, total=len(queries), desc=\"Generating futures\")]\n",
    "            display(len(futures))\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Generating arguments\"):\n",
    "                results.append(future.result())\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def run_experiment(self):\n",
    "        self.load_queries()\n",
    "        print(f\"Loaded: {len(self.queries)} queries\")\n",
    "    \n",
    "        file_path = '../data/results.csv'\n",
    "        pool_args = self.generate_arguments_with_progress(self.queries, self.settings, file_path)\n",
    "\n",
    "        with Pool(int(cpu_count()/2)) as pool:\n",
    "            for _ in tqdm(pool.uimap(process_query, pool_args), total=len(pool_args), desc=\"Queries\", unit=\"query\"):\n",
    "                pass\n",
    "\n",
    "    def load_queries(self):\n",
    "        df_query = pd.read_csv('../data/queries_df.csv')\n",
    "        self.queries = df_query['query'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bdb7f-f9dd-4d29-928f-325f1bf8e6b8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aac17-9cb6-40a5-a467-14f0e478afb2",
   "metadata": {},
   "source": [
    "## 1. Health test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9440d9bd-c137-4d5e-9a8f-1af9eed87a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating futures: 100%|███████████████████████████████████████████████████| 850000/850000 [00:32<00:00, 25919.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating arguments: 100%|████████████████████████████████████████████████| 850000/850000 [00:05<00:00, 148814.53it/s]\n",
      "Queries:   0%|                                                                           | 0/850000 [00:00<?, ?query/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VirtualAggregator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"D:\\Python\\Python311\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Python\\Python311\\Lib\\site-packages\\pathos\\helpers\\mp_helper.py\", line 15, in <lambda>\n    func = lambda args: f(*args)\n                        ^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4856\\2360002701.py\", line 113, in process_query\nNameError: name 'VirtualAggregator' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#set_start_method('spawn')\u001b[39;00m\n\u001b[0;32m     17\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(settings)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mExperiment.run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m pool_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_arguments_with_progress(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings, file_path)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;28mint\u001b[39m(cpu_count()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m---> 28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpool_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQueries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\multiprocess\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VirtualAggregator' is not defined"
     ]
    }
   ],
   "source": [
    "# Parametry wirtualnego agregatora\n",
    "if __name__ == '__main__':\n",
    "    settings = [\n",
    "        {\n",
    "            'N': 20,\n",
    "            'k': 10,\n",
    "            'pn': [0.5, 0.3, 0.1, 0.1],\n",
    "        },\n",
    "        {\n",
    "            'N': 20,\n",
    "            'k': 15,\n",
    "            'pn': [0.5, 0.2, 0.2, 0.1],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    #set_start_method('spawn')\n",
    "    experiment = Experiment(settings)\n",
    "    experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4186-07d9-4707-870d-5111933552fd",
   "metadata": {},
   "source": [
    "## 2. Fill experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ecdf0ea-1650-4919-a5bc-efad413ea86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples_with_fixed_pn(num_examples):\n",
    "    examples = []\n",
    "    for _ in range(num_examples):\n",
    "        N = np.random.randint(10, 30)\n",
    "        k = np.random.randint(5, N)\n",
    "        pn = np.random.dirichlet(np.ones(4), size=1)[0]\n",
    "        pn = np.round(pn, 2).tolist()\n",
    "        examples.append({\n",
    "            'N': N,\n",
    "            'k': k,\n",
    "            'pn': pn,\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "settings = generate_examples_with_fixed_pn(500)\n",
    "#display(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e90856-e23e-4f4f-93a2-c1c99cb8ff52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating futures: 100%|███████████████████████████████████████████████████| 850000/850000 [00:34<00:00, 24485.54it/s]\n",
      "Generating arguments: 100%|████████████████████████████████████████████████| 850000/850000 [00:04<00:00, 170876.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries:   0%|                                                                           | 0/850000 [00:00<?, ?query/s]"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(settings)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa475623-1ba9-4bee-adff-cbae0eb8a3ea",
   "metadata": {},
   "source": [
    "# Read result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cbcf2bf-90be-42df-be98-bd44e3e120e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>settings</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 11, 'k': 8, 'pn': [0.32, 0.0, 0.56, 0.11]}</td>\n",
       "      <td>{'645696': 2, '132239': 1, '700503': 2, '47508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 21, 'k': 13, 'pn': [0.19, 0.53, 0.08, 0....</td>\n",
       "      <td>{'158554': 2, '39779': 1, '565416': 1, '461731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 25, 'k': 19, 'pn': [0.2, 0.5, 0.07, 0.23]}</td>\n",
       "      <td>{'46166': 2, '733536': 1, '461731': 2, '158554...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 12, 'k': 9, 'pn': [0.22, 0.04, 0.02, 0.71]}</td>\n",
       "      <td>{'658043': 2, '132239': 1, '551126': 2, '47508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 23, 'k': 22, 'pn': [0.47, 0.02, 0.25, 0....</td>\n",
       "      <td>{'603747': 2, '780573': 1, '210987': 1, '15855...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0  Is Proxy Record Customizable Manager?   \n",
       "1  Is Proxy Record Customizable Manager?   \n",
       "2  Is Proxy Record Customizable Manager?   \n",
       "3  Is Proxy Record Customizable Manager?   \n",
       "4  Is Proxy Record Customizable Manager?   \n",
       "\n",
       "                                            settings  \\\n",
       "0   {'N': 11, 'k': 8, 'pn': [0.32, 0.0, 0.56, 0.11]}   \n",
       "1  {'N': 21, 'k': 13, 'pn': [0.19, 0.53, 0.08, 0....   \n",
       "2   {'N': 25, 'k': 19, 'pn': [0.2, 0.5, 0.07, 0.23]}   \n",
       "3  {'N': 12, 'k': 9, 'pn': [0.22, 0.04, 0.02, 0.71]}   \n",
       "4  {'N': 23, 'k': 22, 'pn': [0.47, 0.02, 0.25, 0....   \n",
       "\n",
       "                                        distribution  \n",
       "0  {'645696': 2, '132239': 1, '700503': 2, '47508...  \n",
       "1  {'158554': 2, '39779': 1, '565416': 1, '461731...  \n",
       "2  {'46166': 2, '733536': 1, '461731': 2, '158554...  \n",
       "3  {'658043': 2, '132239': 1, '551126': 2, '47508...  \n",
       "4  {'603747': 2, '780573': 1, '210987': 1, '15855...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.read_csv('../data/results.csv')\n",
    "display(df_results.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89321f84-3b10-41f1-91a0-a4cb47c816b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
