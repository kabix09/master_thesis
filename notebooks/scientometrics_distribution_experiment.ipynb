{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01a8400-5267-4dd5-a990-dee8d8f666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec770c-5ba2-4c9d-9f40-48ce2b29d942",
   "metadata": {},
   "source": [
    "## Distribution generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5595ea10-c0d9-4765-8bb4-ea4c04b3b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualAggregator:\n",
    "    \"\"\"\n",
    "    Generates a distribution of selected papers based on specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of citations to sample.\n",
    "    N : int\n",
    "        Page size for pagination.\n",
    "    p : list\n",
    "        List of weights for criteria: [semantic similarity, publication year, number of citations, publication venue].\n",
    "    Q : str\n",
    "        Query used for selecting papers.\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing query results with columns: 'id', 'title', 'similarity', 'year', 'n_citation', 'gov_score'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Counter\n",
    "        Counter object containing identifiers of selected papers and their counts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.collection = None\n",
    "        self.N = None\n",
    "        self.k = None\n",
    "        self.pn = None\n",
    "        self.chroma_collection = None\n",
    "        self.init_connection()\n",
    "\n",
    "    def set_parameters(self, N, k, pn):\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "        self.pn = pn\n",
    "\n",
    "    def init_connection(self):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "                self.chroma_collection = chroma_client.get_or_create_collection(name=\"articles_with_score\")\n",
    "                collection_status = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def get_similar_articles(self, query, max_similarities):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                return self.chroma_collection.query(query_texts=[query], n_results=max_similarities)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def distribution_function(self, page_count):\n",
    "        pages_distribution = np.exp(-np.arange(1, page_count + 1))\n",
    "        pages_distribution /= pages_distribution.sum()\n",
    "        return pages_distribution\n",
    "\n",
    "    def distribution_generator(self, collection_dict):    \n",
    "        values_to_scale = np.array([\n",
    "                collection_dict['year'],\n",
    "                collection_dict['n_citation'],\n",
    "                collection_dict['gov_score']\n",
    "            ]).T\n",
    "\n",
    "        # Dopasowanie i przekształcenie danych\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_values = scaler.fit_transform(values_to_scale)\n",
    "\n",
    "        collection_dict['year_normalized'] = scaled_values[:, 0].tolist()\n",
    "        collection_dict['citations_normalized'] = scaled_values[:, 1].tolist()\n",
    "        collection_dict['points_normalized'] = scaled_values[:, 2].tolist()\n",
    "\n",
    "        collection_dict['score'] = [\n",
    "            self.pn[0] * collection_dict['similarity'][i] +\n",
    "            self.pn[1] * collection_dict['year_normalized'][i] +\n",
    "            self.pn[2] * collection_dict['citations_normalized'][i] +\n",
    "            self.pn[3] * collection_dict['points_normalized'][i]\n",
    "            for i in range(len(collection_dict['id']))\n",
    "        ]\n",
    "\n",
    "        # Tworzenie listy słowników dla posortowania\n",
    "        sorted_collection = sorted(\n",
    "            [\n",
    "                {\n",
    "                    'id': collection_dict['id'][i],\n",
    "                    'title': collection_dict['title'][i],\n",
    "                    'similarity': collection_dict['similarity'][i],\n",
    "                    'year': collection_dict['year'][i],\n",
    "                    'n_citation': collection_dict['n_citation'][i],\n",
    "                    'gov_score': collection_dict['gov_score'][i],\n",
    "                    'year_normalized': collection_dict['year_normalized'][i],\n",
    "                    'citations_normalized': collection_dict['citations_normalized'][i],\n",
    "                    'points_normalized': collection_dict['points_normalized'][i],\n",
    "                    'score': collection_dict['score'][i]\n",
    "                }\n",
    "                for i in range(len(collection_dict['id']))\n",
    "            ],\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Stronicowanie wyników\n",
    "        ranked_indices = [entry['id'] for entry in sorted_collection]\n",
    "        pages = [ranked_indices[i:i + self.N] for i in range(0, len(ranked_indices), self.N)]\n",
    "        pages_distribution = self.distribution_function(len(pages))\n",
    "        \n",
    "        # Losowanie k prac\n",
    "        np.random.seed(42)  # Ustawienie ziarna losowości dla powtarzalności wyników\n",
    "\n",
    "        selected_papers = []\n",
    "        for _ in range(self.k):\n",
    "            # Problem pustej strony - pojawia sie kiedy zdejmiemy juz wszytskei dostepne artykuły z tej strony w drodze losowania bez powtórzeń\n",
    "            non_empty_pages = [page for page in pages if len(page) > 0]\n",
    "            non_empty_distribution = self.distribution_function(len(non_empty_pages))\n",
    "            \n",
    "            selected_page_index = np.random.choice(len(non_empty_pages), p=non_empty_distribution)\n",
    "            selected_page = non_empty_pages[selected_page_index]                \n",
    "            selected_paper_index = np.random.choice(selected_page)\n",
    "            selected_papers.append(selected_paper_index)\n",
    "\n",
    "            # Usuwanie wylosowanych wyników\n",
    "            pages[selected_page_index] = [x for x in selected_page if x != selected_paper_index]\n",
    "\n",
    "        # Zapisanie identyfikatorów wylosowanych prac\n",
    "        selected_paper_counts = collections.Counter(selected_papers)\n",
    "\n",
    "        return selected_paper_counts\n",
    "\n",
    "    def select_papers(self, ranking):\n",
    "        selected_papers = random.sample(ranking, self.k)\n",
    "        return selected_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32263551-a67c-4fe7-81e6-b65aca0c610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, settings):\n",
    "        self.virtual_aggregator = VirtualAggregator()\n",
    "        self.queries = None\n",
    "        self.settings = settings\n",
    "        self.similar_articles = None\n",
    "\n",
    "    def run_experiment(self):\n",
    "        self.load_queries()\n",
    "        print(f\"Loaded: {len(self.queries)} queries\")\n",
    "\n",
    "        distribution_dict = {}\n",
    "\n",
    "        counter = 0\n",
    "        result_dict = {\n",
    "            'query_id': [],\n",
    "            'settings': [],\n",
    "            'distribution': [],\n",
    "        }\n",
    "\n",
    "        for i, query in enumerate(tqdm(self.queries, total=len(self.queries), desc=\"Queries\", unit=\"query\")):\n",
    "            self.similar_articles = self.virtual_aggregator.get_similar_articles(query, 250)\n",
    "\n",
    "            for sample in self.settings:\n",
    "                self.virtual_aggregator.set_parameters(sample['N'], sample['k'], sample['pn'])\n",
    "                step_distribution = self.step(query)\n",
    "\n",
    "                # Save result\n",
    "                result_dict['query_id'].append(i)\n",
    "                result_dict['settings'].append(sample)\n",
    "                result_dict['distribution'].append(dict(step_distribution))\n",
    "\n",
    "                if str(sample) in distribution_dict:                  \n",
    "                    distribution_dict[str(sample)].update(step_distribution)\n",
    "                else:\n",
    "                    distribution_dict[str(sample)] = step_distribution\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "            if counter % 300 == 0:\n",
    "                self.save_distribution(distribution_dict)\n",
    "                self.save_results(result_dict)\n",
    "                result_dict = {\n",
    "                    'query_id': [],\n",
    "                    'settings': [],\n",
    "                    'distribution': [],\n",
    "                }\n",
    "\n",
    "        self.save_results(result_dict)\n",
    "        self.save_distribution(distribution_dict)\n",
    "\n",
    "    def step(self, query):\n",
    "        collection_dict = {\n",
    "            'id': self.similar_articles['ids'][0],\n",
    "            'title': self.similar_articles['documents'][0],\n",
    "            'similarity': self.similar_articles['distances'][0],\n",
    "            'year': [metadata['year'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'n_citation': [metadata['n_citation'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'gov_score': [metadata['gov_score'] for metadata in self.similar_articles['metadatas'][0]]\n",
    "        }\n",
    "\n",
    "        return self.virtual_aggregator.distribution_generator(collection_dict)\n",
    "\n",
    "    def load_queries(self):\n",
    "        df_query = pd.read_csv('../data/queries_df.csv')\n",
    "        self.queries = df_query['Query'].tolist()\n",
    "\n",
    "    def save_results(self, result_dict):\n",
    "        #results_df.to_csv('../data/results.csv', index=False)\n",
    "        # Zapisanie słownika do pliku CSV\n",
    "        file_exists = os.path.isfile('../data/results.csv')\n",
    "        keys = result_dict.keys()\n",
    "        with open('../data/results.csv', 'a', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "            if not file_exists:\n",
    "                dict_writer.writeheader()  # Zapis nagłówków tylko, gdy plik nie istnieje\n",
    "            dict_writer.writerows([dict(zip(keys, row)) for row in zip(*result_dict.values())])\n",
    "\n",
    "    def save_distribution(self, distribution_dict):\n",
    "        distribution_df = pd.DataFrame(list(distribution_dict.items()), columns=['settings', 'distribution'])\n",
    "        distribution_df['distribution'] = distribution_df['distribution'].apply(lambda x: dict(x))\n",
    "        distribution_df.to_csv('../distributions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecdf0ea-1650-4919-a5bc-efad413ea86b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_all_settings():\n",
    "    page_sizes = [10, 100]\n",
    "    citation_numbers = [10, 25, 50]\n",
    "    weights = [0., 0.1, 0.25, 0.33, 0.5, 0.75, 0.9, 1.0]\n",
    "\n",
    "    all_combinations = list(itertools.product(weights, repeat=4))\n",
    "\n",
    "    valid_configs = [list(c) for c in all_combinations if 0.99 <= sum(c) <= 1.0]\n",
    "\n",
    "    # Generowanie wszystkich możliwych ustawień\n",
    "    settings = []\n",
    "    for page_size in page_sizes:\n",
    "        for citation_number in citation_numbers:\n",
    "            for config in valid_configs:\n",
    "                settings.append({\n",
    "                    'N': page_size,\n",
    "                    'k': citation_number,\n",
    "                    'pn': config\n",
    "                })\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bdb7f-f9dd-4d29-928f-325f1bf8e6b8",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c0a0c-e377-4e04-92e9-fe3fb4be418b",
   "metadata": {},
   "source": [
    "### 1. Generate settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8a7bdc-6579-4c5f-987e-054513815bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wygenerowanych konfiguracji:  306\n"
     ]
    }
   ],
   "source": [
    "settings = generate_all_settings()\n",
    "display(settings[0])\n",
    "print(\"Liczba wygenerowanych konfiguracji: \", len(settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29564f8c-3214-4449-8aeb-ab266bd80365",
   "metadata": {},
   "source": [
    "### 2. Run main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3e90856-e23e-4f4f-93a2-c1c99cb8ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries:   0%|▏                                                           | 2108/850000 [18:39<125:02:44,  1.88query/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(settings)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36mExperiment.run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_aggregator\u001b[38;5;241m.\u001b[39mset_parameters(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 26\u001b[0m     step_distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Save result\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "Cell \u001b[1;32mIn[24], line 62\u001b[0m, in \u001b[0;36mExperiment.step\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, query):\n\u001b[0;32m     53\u001b[0m     collection_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilar_articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilar_articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgov_score\u001b[39m\u001b[38;5;124m'\u001b[39m: [metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgov_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m metadata \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilar_articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     60\u001b[0m     }\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_aggregator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 137\u001b[0m, in \u001b[0;36mVirtualAggregator.distribution_generator\u001b[1;34m(self, collection_dict)\u001b[0m\n\u001b[0;32m    134\u001b[0m non_empty_pages \u001b[38;5;241m=\u001b[39m [page \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(page) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    135\u001b[0m non_empty_distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_function(\u001b[38;5;28mlen\u001b[39m(non_empty_pages))\n\u001b[1;32m--> 137\u001b[0m selected_page_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(non_empty_pages), p\u001b[38;5;241m=\u001b[39mnon_empty_distribution)\n\u001b[0;32m    138\u001b[0m selected_page \u001b[38;5;241m=\u001b[39m non_empty_pages[selected_page_index]                \n\u001b[0;32m    139\u001b[0m selected_paper_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(selected_page)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment = Experiment(settings)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa475623-1ba9-4bee-adff-cbae0eb8a3ea",
   "metadata": {},
   "source": [
    "### 3. Read result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbcf2bf-90be-42df-be98-bd44e3e120e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>settings</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}</td>\n",
       "      <td>{'358372': 1, '228408': 1, '261158': 1, '19472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.1, 0.9]}</td>\n",
       "      <td>{'694324': 1, '453823': 1, '444086': 1, '56808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.25, 0.75]}</td>\n",
       "      <td>{'694324': 1, '453823': 1, '444086': 1, '56808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.5, 0.5]}</td>\n",
       "      <td>{'453823': 1, '541503': 1, '694324': 1, '19472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.75, 0.25]}</td>\n",
       "      <td>{'70014': 1, '257451': 1, '541503': 1, '194723...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                          settings  \\\n",
       "0       150    {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}   \n",
       "1       150    {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.1, 0.9]}   \n",
       "2       150  {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.25, 0.75]}   \n",
       "3       150    {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.5, 0.5]}   \n",
       "4       150  {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.75, 0.25]}   \n",
       "\n",
       "                                        distribution  \n",
       "0  {'358372': 1, '228408': 1, '261158': 1, '19472...  \n",
       "1  {'694324': 1, '453823': 1, '444086': 1, '56808...  \n",
       "2  {'694324': 1, '453823': 1, '444086': 1, '56808...  \n",
       "3  {'453823': 1, '541503': 1, '694324': 1, '19472...  \n",
       "4  {'70014': 1, '257451': 1, '541503': 1, '194723...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_df = pd.read_csv('../data/results.csv')\n",
    "display(distribution_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568afe8c-5999-4829-bcf5-f6fe9d5e6083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
